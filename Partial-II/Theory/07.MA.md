---
tags:
  - Python
  - time-series
---
# Introduction
In the [[05.AR]] (`Partial-II/Theory/05.AR.md`) we made a regression based on the lags of the time series.
$$y_t = \phi_0 + \sum_{i=1}^p \phi_i y_{t-i} + \varepsilon_t$$
where $p$ is the order or the number of past observations.

In the Moving Average model (**MA**) we are going to make a regression but based in the *past errors* (See **Insight 1**).
# Definition
MA is a statistical model to forecast trends and understand patterns in the time series. The goal is to model the present value as a linear combination of the past white noise error terms (errors from the past!) of the time series. 
## Formula
$$y_t = \mu + \sum_{i=1}^q \theta_i \varepsilon_{t-i} + \varepsilon_t$$

where:
- $\mu$ is the mean of the time series (See **Insight 2**)
- $\varepsilon_i$ is the error in the $i$-nth lag
- $q$ is the number of past errors (also now as **order** of $MA$ model)
- $\theta_i$ are the estimators (constants) of the model

# Long vs Short Memory
## Long memory models
The AR model is considered as a model with **Long Memory** due to the formula is recursive.

$AR(1)$ **for today**

$$y_t = \omega + \phi y_{t-1} + \varepsilon_{t}$$

$AR(1)$ **for yesterday**

$$y_{t-1} = \omega^\prime + \phi^\prime y_{t-2} + \varepsilon_{t-1}$$

Then

$$y_t = \omega + \phi (\omega^\prime + \phi^\prime y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t}$$

because

$$
\begin{align}
y_t = & \omega + \phi y_{t-1} + \varepsilon_{t} \\
= & \omega + \phi (\omega^\prime + \phi^\prime y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t} \\
= & \omega + \phi\omega^\prime + \phi\phi^\prime y_{t-2} + \phi\varepsilon_{t-1} + \varepsilon_{t}
\end{align} 
$$

making $\phi_0 := \omega + \phi\omega$, $\phi_1 := \phi\phi^\prime$, and $\phi := \theta$

$$
\begin{align}
y_t = & \omega + \phi\omega^\prime + \phi\phi^\prime y_{t-2} + \phi\varepsilon_{t-1} + \varepsilon_{t} \\
= & \phi_0 + \phi_1 y_{t-2} + \phi\varepsilon_{t-1} + \varepsilon_{t} \\
= & \phi_0 + \phi_1 y_{t-2} + \theta\varepsilon_{t-1} + \varepsilon_{t}
\end{align} 
$$


## Short memory models
MA model is considered a **Short Memory** model because the errors don't last long into the future.
### $MA(1)$ from yesterday
$$y_{t-1} = \mu + \theta\varepsilon_{t-2} + \varepsilon_{t-1}$$
**Read as**: prediction of *yesterday* ($y_{t-1}$) depends on the error from *yesterday* ($\varepsilon_{t-1}$) and some error from the *day before yesterday* ($\varepsilon_{t-2}$)
### $MA(1)$ from today
$$y_{t} = \mu + \theta\varepsilon_{t-1} + \varepsilon_{t}$$
**Read as**: prediction of *today* ($y_{t}$) depends on the error from *today* ($\varepsilon_{t}$) and some error from *yesterday* ($\varepsilon_{t-1}$)

- **Note**: There is not more error from the day before yesterday! ($\varepsilon_{t-2}$)
### $MA(1)$ from tomorrow
$$y_{t + 1} = \mu + \theta\varepsilon_{t} + \varepsilon_{t + 1}$$
**Read as**: prediction of *tomorrow* ($y_{t+1}$) depends on the error from *tomorrow* ($\varepsilon_{t+1}$) and some error from *today* ($\varepsilon_{t}$)

- **Note**: There is not more error from yesterday! ($\varepsilon_{t-1}$)

# Best Model
Some model that used both Long an Short memory i.e. **ARIMA**.

# Insights
1. Why errors?
- Given a forecast there are some errors
- The errors that happen before affect the current observations

1. Why $\mu$ is the prediction for the first value?
- Because you don't have previous errors to estimate